{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPPVdvRN-sKC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import optuna\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import optuna\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Attention, Concatenate, Reshape\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import joblib\n",
        "from prophet import Prophet\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import optuna\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import optuna\n",
        "import joblib\n",
        "import xgboost as xgb\n",
        "import shap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "file_path = '/content/lstm_exch.xlsx'\n",
        "df = pd.read_excel(file_path, sheet_name='Exch')\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# --------------------------\n",
        "# Feature Engineering (no leakage)\n",
        "# --------------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from prophet import Prophet\n",
        "\n",
        "def feature_engineering(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Ensure datetime format\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    df.sort_values('Date', inplace=True)\n",
        "\n",
        "    # Lag Features\n",
        "    df['USD/PKR_lag1'] = df['USD/PKR'].shift(1)\n",
        "    df['USD/PKR_lag3'] = df['USD/PKR'].shift(3)\n",
        "    df['USD/PKR_lag7'] = df['USD/PKR'].shift(7)\n",
        "    df['USD_EUR_lag1'] = df['USD/EUR'].shift(1)\n",
        "    df['USD_EUR_lag2'] = df['USD/EUR'].shift(3)\n",
        "    df['USD_JPY_lag3'] = df['USD/JPY'].shift(1)\n",
        "\n",
        "\n",
        "    # Rolling statistics\n",
        "    df['roll_mean_3'] = df['USD/PKR'].rolling(window=3).mean()\n",
        "    df['roll_std_3'] = df['USD/PKR'].rolling(window=3).std()\n",
        "\n",
        "    # Momentum\n",
        "    df['pkr_diff'] = df['USD/PKR'].diff()\n",
        "    df['pkr_pct'] = df['USD/PKR'].pct_change()\n",
        "\n",
        "    # Other currency relationships\n",
        "    df['usd_jpy_diff'] = df['USD/JPY'].diff()\n",
        "    df['usd_eur_diff'] = df['USD/EUR'].diff()\n",
        "\n",
        "    # Time-based\n",
        "    df['days_since_start'] = (df['Date'] - df['Date'].min()).dt.days\n",
        "\n",
        "    df['month'] = df['Date'].dt.month\n",
        "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "\n",
        "    # Prophet forecast\n",
        "    prophet_df = df[['Date', 'USD/PKR']].rename(columns={'Date': 'ds', 'USD/PKR': 'y'})\n",
        "    prophet_model = Prophet(yearly_seasonality=True, daily_seasonality=False)\n",
        "    prophet_model.fit(prophet_df)\n",
        "    forecast = prophet_model.predict(prophet_df)\n",
        "    df['prophet_pred'] = forecast['yhat'].shift(1)\n",
        "    df['prophet_residual'] = df['USD/PKR'] - df['prophet_pred'].shift(1)\n",
        "\n",
        "    # Momentum\n",
        "    df['pkr_momentum_7'] = df['USD/PKR'] - df['USD/PKR'].shift(7)\n",
        "\n",
        "# Then include 'prophet_pred_lagged' in your features before scaling and building sequences\n",
        "   # Drop rows with any NaN values\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    return df\n",
        "\"\"\"\n",
        "df = feature_engineering(df)\n",
        "\n",
        "# ------------------ 1. Prepare the data ------------------\n",
        "y = df['USD/PKR']  # Target column\n",
        "X = df.drop(['USD/PKR','Date','month'], axis=1)  # Feature columns\n",
        "\n",
        "# Step 1: Remove Multicollinearity\n",
        "def remove_multicollinearity(df, threshold=0.95):\n",
        "    corr = df.corr().abs()\n",
        "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
        "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
        "    print(f\"Removed {len(to_drop)} highly correlated features: {to_drop}\")\n",
        "    return df.drop(columns=to_drop), to_drop\n",
        "\n",
        "# Step 2: Compare SHAP Importances\n",
        "def compare_shap_importances(X, y, top_n=10):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # XGBoost model + SHAP\n",
        "    xgb_model = xgb.XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42)\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "    explainer_xgb = shap.Explainer(xgb_model)  # Uses TreeExplainer by default\n",
        "    shap_values_xgb = explainer_xgb(X_train)\n",
        "    xgb_shap = pd.DataFrame({\n",
        "        'feature': X.columns,\n",
        "        'xgb_mean_abs_shap': np.abs(shap_values_xgb.values).mean(axis=0)\n",
        "    })\n",
        "\n",
        "    # Neural Network (MLP)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    nn_model = MLPRegressor(hidden_layer_sizes=(64, 64), max_iter=500, random_state=42)\n",
        "    nn_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # SHAP for NN (KernelExplainer - expensive, so sample)\n",
        "    sample_X = shap.sample(X_train_scaled, 100, random_state=42)\n",
        "    explainer_nn = shap.KernelExplainer(nn_model.predict, sample_X)\n",
        "    shap_values_nn = explainer_nn.shap_values(X_train_scaled[:50], nsamples=100)\n",
        "\n",
        "    nn_shap = pd.DataFrame({\n",
        "        'feature': X.columns,\n",
        "        'nn_mean_abs_shap': np.abs(shap_values_nn).mean(axis=0)\n",
        "    })\n",
        "\n",
        "    # Merge importances\n",
        "    comparison_df = pd.merge(xgb_shap, nn_shap, on='feature')\n",
        "    comparison_df = comparison_df.sort_values('xgb_mean_abs_shap', ascending=False)\n",
        "\n",
        "    # Plot\n",
        "    comparison_df.set_index('feature')[['xgb_mean_abs_shap', 'nn_mean_abs_shap']].head(top_n).plot(\n",
        "        kind='barh', figsize=(10, 6), title='XGBoost vs NN SHAP Importance'\n",
        "    )\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return comparison_df\n",
        "\n",
        "# Step 3: Select Top Features\n",
        "def select_top_features(shap_df, top_k=10, method='average'):\n",
        "    if method == 'average':\n",
        "        shap_df['avg_shap'] = shap_df[['xgb_mean_abs_shap', 'nn_mean_abs_shap']].mean(axis=1)\n",
        "        top_feats = shap_df.sort_values('avg_shap', ascending=False)['feature'].head(top_k).tolist()\n",
        "    elif method == 'intersection':\n",
        "        top_xgb = shap_df.sort_values('xgb_mean_abs_shap', ascending=False)['feature'].head(top_k)\n",
        "        top_nn = shap_df.sort_values('nn_mean_abs_shap', ascending=False)['feature'].head(top_k)\n",
        "        top_feats = list(set(top_xgb).intersection(set(top_nn)))\n",
        "    else:\n",
        "        raise ValueError(\"Method must be 'average' or 'intersection'\")\n",
        "\n",
        "    print(f\"\\nSelected top {len(top_feats)} features ({method}): {top_feats}\")\n",
        "    return top_feats\n",
        "\n",
        "# Final Pipeline\n",
        "def shap_feature_selection_pipeline(X, y, multicollinearity_threshold=0.95, top_k=10, method='average'):\n",
        "    print(\"\\n--- Running SHAP-Based Feature Selection Pipeline ---\")\n",
        "\n",
        "    # Step 1: Multicollinearity\n",
        "    X_reduced, dropped_cols = remove_multicollinearity(X, threshold=multicollinearity_threshold)\n",
        "\n",
        "    # Step 2: SHAP comparison\n",
        "    shap_df = compare_shap_importances(X_reduced, y, top_n=top_k)\n",
        "\n",
        "    # Step 3: Feature selection\n",
        "    selected_features = select_top_features(shap_df, top_k=top_k, method=method)\n",
        "\n",
        "    # Return selected dataset\n",
        "    return X_reduced[selected_features], selected_features, shap_df, dropped_cols\n",
        "# RUN\n",
        "X_selected, selected_feats, shap_df, dropped = shap_feature_selection_pipeline(\n",
        "    X, y,\n",
        "    multicollinearity_threshold=0.95,\n",
        "    top_k=10,\n",
        "    method='average'\n",
        ")\n",
        "\n",
        "# -- MODELING --\n",
        "\n",
        "df.sort_values('Date', inplace=True)\n",
        "df.drop('Date',axis=1)\n",
        "\n",
        "# Selecting features\n",
        "data = df[[\n",
        "    'USD/PKR',\n",
        "    'USD/PKR_lag1',\n",
        "    'USD/JPY',\n",
        "    'USD/EUR',\n",
        "    'days_since_start'\n",
        "]].astype(float)\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Create sequences\n",
        "def create_sequences(data, lookback):\n",
        "    X, y = [], []\n",
        "    for i in range(lookback, len(data)):\n",
        "        X.append(data[i - lookback:i])\n",
        "        y.append(data[i, 0])  # Target is USD/PKR\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# 1.37 best_params = {'batch_size': 16, 'num_layers': 1, 'lookback': 70, 'units': 72, 'dropout': 0.015182762252143527, 'lr': 0.004446771977416189, 'l2_reg_lstm': 1.2715967548965089e-06, 'early_patience': 10, 'early_min_delta': 6.23793908071683e-05, 'reduce_factor': 0.8880892095511763, 'reduce_patience': 3, 'min_lr': 5.595165634673799e-06}\n",
        "# 1.39 {'batch_size': 16, 'num_layers': 1, 'lookback': 95, 'units': 72, 'dropout': 0.020354140678498382, 'lr': 0.007055226563672889, 'l2_reg_lstm': 7.540667771887033e-07, 'early_patience': 10, 'early_min_delta': 1.1366211591681307e-05, 'reduce_factor': 0.7841208938505344, 'reduce_patience': 2, 'min_lr': 3.774349882958255e-05}\n",
        "# value 1.41 best_params = {'batch_size': 16, 'num_layers': 1, 'lookback': 70, 'units': 72, 'dropout': 0.015182762252143527, 'lr': 0.004446771977416189, 'l2_reg_lstm': 1.2715967548965089e-06, 'early_patience': 10, 'early_min_delta': 6.23793908071683e-05, 'reduce_factor': 0.8880892095511763, 'reduce_patience': 3, 'min_lr': 5.595165634673799e-06}\n",
        "# value: 1.40  {'batch_size': 16, 'num_layers': 1, 'lookback': 95, 'units': 72, 'dropout': 0.020354140678498382, 'lr': 0.007055226563672889, 'l2_reg_lstm': 7.540667771887033e-07, 'early_patience': 10, 'early_min_delta': 1.1366211591681307e-05, 'reduce_factor': 0.7841208938505344, 'reduce_patience': 2, 'min_lr': 3.774349882958255e-05}\n",
        "\n",
        "# value: 1.3768831331105493 and parameters: {'batch_size': 16, 'num_layers': 1, 'lookback': 108, 'units': 57, 'dropout': 0.011810305033639949, 'lr': 0.006185124160360622, 'l2_reg_lstm': 2.0623626782630574e-06, 'early_patience': 13, 'early_min_delta': 5.408021753464203e-05, 'reduce_factor': 0.6632453018415295, 'reduce_patience': 3, 'min_lr': 4.200249724501052e-06}\n",
        "def objective(trial):\n",
        "    # Hyperparameters\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16])\n",
        "    num_layers = trial.suggest_int('num_layers', 1,1)\n",
        "    lookback = trial.suggest_int('lookback', 65,120 )\n",
        "    units = trial.suggest_int('units', 55,75)\n",
        "    dropout = trial.suggest_float('dropout',  0.01, 0.1)\n",
        "    lr = trial.suggest_float('lr', 0.004, 0.09, log=True)\n",
        "    l2_reg_lstm = trial.suggest_float('l2_reg_lstm', 1e-7, 1e-6, log=True)\n",
        "    #l2_reg_dense = trial.suggest_float('l2_reg_dense', 1e-8, 1e-4, log=True)\n",
        "\n",
        "\n",
        "    # Callbacks tuning\n",
        "    early_patience = trial.suggest_int(\"early_patience\", 5, 10)\n",
        "    early_min_delta = trial.suggest_float(\"early_min_delta\", 1e-5, 1e-2, log=True)\n",
        "    reduce_factor = trial.suggest_float(\"reduce_factor\", 0.1, 0.9)\n",
        "    reduce_patience = trial.suggest_int(\"reduce_patience\", 1,3 )\n",
        "    min_lr = trial.suggest_float(\"min_lr\", 1e-6, 1e-4, log=True)\n",
        "\n",
        "    # Prepare data\n",
        "    X, y = create_sequences(scaled_data, lookback)\n",
        "    train_size = int(len(X) * 0.8)\n",
        "    X_train, X_test = X[:train_size], X[train_size:]\n",
        "    y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "    # Build model\n",
        "    inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
        "    x = inputs\n",
        "    for i in range(num_layers):\n",
        "        return_seq =  i < num_layers - 1\n",
        "        x = LSTM(units=units, return_sequences=return_seq,\n",
        "                 kernel_regularizer=l2(l2_reg_lstm))(x)\n",
        "        if dropout > 0:\n",
        "            x = Dropout(dropout)(x)\n",
        "    output = Dense(1)(x)\n",
        "    model = Model(inputs, output)\n",
        "    model.compile(optimizer=Adam(learning_rate=lr), loss='mae')\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=early_patience,\n",
        "                      restore_best_weights=True, min_delta=early_min_delta),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=reduce_factor,\n",
        "                          patience=reduce_patience, min_lr=min_lr)\n",
        "    ]\n",
        "\n",
        "    model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
        "              epochs=50, batch_size=batch_size, verbose=1, callbacks=callbacks)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_inv = scaler.inverse_transform(np.concatenate((y_pred, X_test[:, -1, 1:]), axis=1))[:, 0]\n",
        "    y_test_inv = scaler.inverse_transform(np.concatenate((y_test.reshape(-1, 1), X_test[:, -1, 1:]), axis=1))[:, 0]\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))\n",
        "    return rmse\n",
        "\n",
        "# Run optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "print(\"Best Hyperparameters:\", study.best_params)\n",
        "print(\"Best RMSE:\", study.best_value)\n",
        "\n",
        "# Extract best parameters\n",
        "best_params = study.best_params\n",
        "\n",
        "# Recreate sequences\n",
        "lookback = best_params['lookback']\n",
        "X, y = create_sequences(scaled_data, lookback)\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Build model with or without attention\n",
        "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
        "x = inputs\n",
        "for i in range(best_params['num_layers']):\n",
        "    return_seq =  i < best_params['num_layers'] - 1\n",
        "    x = LSTM(units=best_params['units'], return_sequences=return_seq,\n",
        "             kernel_regularizer=l2(best_params['l2_reg_lstm']))(x)\n",
        "    if best_params['dropout'] > 0:\n",
        "        x = Dropout(best_params['dropout'])(x)\n",
        "output = Dense(1)(x)\n",
        "final_model = Model(inputs, output)\n",
        "final_model.compile(optimizer=Adam(learning_rate=best_params['lr']), loss='mae')\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=best_params['early_patience'],\n",
        "                  restore_best_weights=True, min_delta=best_params['early_min_delta']),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=best_params['reduce_factor'],\n",
        "                      patience=best_params['reduce_patience'], min_lr=best_params['min_lr'])\n",
        "]\n",
        "\n",
        "# Train\n",
        "history = final_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=100,\n",
        "    batch_size=best_params['batch_size'],\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "# === PREDICTION on TEST ===\n",
        "y_pred_test = final_model.predict(X_test)\n",
        "y_pred_test_inv = scaler.inverse_transform(np.concatenate((y_pred_test, X_test[:, -1, 1:]), axis=1))[:, 0]\n",
        "y_test_inv = scaler.inverse_transform(np.concatenate((y_test.reshape(-1, 1), X_test[:, -1, 1:]), axis=1))[:, 0]\n",
        "\n",
        "# === PREDICTION on TRAIN ===\n",
        "y_pred_train = final_model.predict(X_train)\n",
        "y_pred_train_inv = scaler.inverse_transform(np.concatenate((y_pred_train, X_train[:, -1, 1:]), axis=1))[:, 0]\n",
        "y_train_inv = scaler.inverse_transform(np.concatenate((y_train.reshape(-1, 1), X_train[:, -1, 1:]), axis=1))[:, 0]\n",
        "\n",
        "# === METRICS ===\n",
        "rmse_test = np.sqrt(mean_squared_error(y_test_inv, y_pred_test_inv))\n",
        "mae_test = mean_absolute_error(y_test_inv, y_pred_test_inv)\n",
        "\n",
        "rmse_train = np.sqrt(mean_squared_error(y_train_inv, y_pred_train_inv))\n",
        "mae_train = mean_absolute_error(y_train_inv, y_pred_train_inv)\n",
        "\n",
        "print(f\"Test RMSE: {rmse_test:.4f}\")\n",
        "print(f\"Test MAE : {mae_test:.4f}\")\n",
        "print(f\"Train RMSE: {rmse_train:.4f}\")\n",
        "print(f\"Train MAE : {mae_train:.4f}\")\n",
        "\n",
        "# === SAVE FINAL MODEL ===\n",
        "final_model.save(\"final_lstm_model.h5\")\n",
        "joblib.dump(scaler, \"scaler.save\")\n",
        "\n",
        "# === LOSS PLOT ===\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title(\"Loss Curve\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MAE Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"loss_curve.png\")\n",
        "plt.show()\n",
        "\n",
        "# === PLOT PREDICTIONS: TEST ===\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_test_inv, label='Actual')\n",
        "plt.plot(y_pred_test_inv, label='Predicted')\n",
        "plt.title(\"Test Set: Predicted vs Actual USD/PKR\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"USD/PKR Rate\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"prediction_vs_actual_test.png\")\n",
        "plt.show()\n",
        "\n",
        "# === PLOT PREDICTIONS: TRAIN ===\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_train_inv, label='Actual')\n",
        "plt.plot(y_pred_train_inv, label='Predicted')\n",
        "plt.title(\"Train Set: Predicted vs Actual USD/PKR\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"USD/PKR Rate\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"prediction_vs_actual_train.png\")\n",
        "plt.show()\n",
        "\n",
        "# Future Prediction\n",
        "def predict_future(model, data, lookback, steps, scaler):\n",
        "    recent_sequence = data[-lookback:]\n",
        "    predictions = []\n",
        "    for _ in range(steps):\n",
        "        input_seq = np.expand_dims(recent_sequence, axis=0)\n",
        "        pred_scaled = model.predict(input_seq, verbose=0)[0][0]\n",
        "        full_pred = np.concatenate([[pred_scaled], recent_sequence[-1, 1:]])\n",
        "        inv_pred = scaler.inverse_transform([full_pred])[0][0]\n",
        "        predictions.append(inv_pred)\n",
        "        next_step = np.append(pred_scaled, recent_sequence[-1, 1:])\n",
        "        recent_sequence = np.vstack((recent_sequence[1:], next_step))\n",
        "    return predictions\n",
        "\n",
        "future_values = predict_future(final_model, scaled_data, lookback, steps=7, scaler=scaler)\n",
        "print(\"Future Predictions (USD/PKR):\", future_values)"
      ]
    }
  ]
}
